{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Wd-2v9xuhwSB",
        "rJmcths-RkcG",
        "h1_s43BSaUli"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Representations\n",
        "\n",
        "In Natural Language Processing (NLP), word representations are ways to numerically represent words so computers can understand and process them. Think of it like translating words into a language computers speak - numbers!"
      ],
      "metadata": {
        "id": "HalKRApjgZJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "common methods:\n",
        "\n",
        "1. **Traditional Methods:**\n",
        "*One-Hot Encoding:* Each word becomes a unique vector with one \"1\" (representing that word) and the rest filled with zeros. Simple, but doesn't capture relationships between words.\n",
        "TF-IDF: Considers how frequently a word appears in a document (Term Frequency) and how rare it is across all documents (Inverse Document Frequency). Helps identify important words in a context.\n",
        "\n",
        "2. **Word Embeddings:**\n",
        "*Word2Vec*: Learns to predict words that appear near each other in a large text corpus. Represents words as dense vectors, capturing semantic relationships (e.g., \"king\" - \"man\" + \"woman\" â‰ˆ \"queen\").\n",
        "GloVe: Similar to Word2Vec, but also considers global word co-occurrence statistics.\n",
        "\n",
        "**Why Word Embeddings Matter:**\n",
        "\n",
        "Dimensionality Reduction: Representing words in a lower-dimensional space makes computation more efficient.\n",
        "\n",
        "Semantic Capture: Words with similar meanings have similar vector representations, allowing models to understand relationships and analogies.\n",
        "\n",
        "Transfer Learning: Pre-trained word embeddings can be used in different NLP tasks, saving time and resources.\n",
        "\n",
        "In a nutshell, word representations are essential for NLP tasks like machine translation, sentiment analysis, and text classification, enabling computers to understand the meaning and context of human language."
      ],
      "metadata": {
        "id": "NsdQcgjZgdy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag of Words**\n",
        "\n",
        "The Bag-of-Words (BoW) model is a way to represent text data for machine learning algorithms. It's like putting all the words from a document into a bag and counting how many times each word appears, ignoring the order or grammar."
      ],
      "metadata": {
        "id": "Wd-2v9xuhwSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you have two sentences:\n",
        "\n",
        "\"The cat sat on the mat.\"\n",
        "\"The dog chased the cat.\"\n",
        "Steps to create a BoW representation:\n",
        "\n",
        "Vocabulary: Combine all unique words from both sentences: {\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"chased\"}.\n",
        "\n",
        "Counting: For each sentence, count how many times each vocabulary word appears:\n",
        "Sentence 1: {\"the\": 2, \"cat\": 1, \"sat\": 1, \"on\": 1, \"mat\": 1, \"dog\": 0, \"chased\": 0}\n",
        "\n",
        "Sentence 2: {\"the\": 2, \"cat\": 1, \"sat\": 0, \"on\": 0, \"mat\": 0, \"dog\": 1, \"chased\": 1}\n",
        "\n",
        "Result: Each sentence is now represented as a vector of word counts. This numerical representation can be used as input for machine learning algorithms.\n",
        "\n",
        "Key points:\n",
        "**BoW** ignores word order and grammar.\n",
        "It focuses on word frequency.\n",
        "It's simple and efficient for basic text analysis tasks."
      ],
      "metadata": {
        "id": "KFliuoEqjrDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog chased the cat.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print()\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXvf2MxFh0MD",
        "outputId": "2c800dfc-33e4-456c-f92f-276b7befd150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat' 'chased' 'dog' 'mat' 'on' 'sat' 'the']\n",
            "\n",
            "[[1 0 0 1 1 1 2]\n",
            " [1 1 1 0 0 0 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will output the BoW representation of the two sentences as a numerical matrix.\n",
        "\n",
        "**Limitations**:\n",
        "\n",
        "BoW loses information about word order and context.\n",
        "It doesn't capture semantic relationships between words.\n",
        "Despite its limitations, BoW is a useful tool for basic text analysis tasks and can be a starting point for more advanced NLP techniques."
      ],
      "metadata": {
        "id": "XviItzU_j2lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tydAikyKj1jp",
        "outputId": "89970c81-9306-4e0f-ea4b-329da8b1a793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat' 'chased' 'dog' 'mat' 'on' 'sat' 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TF - IDF(term Frequency - Inverse Document Frequency)**\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used in Natural Language Processing (NLP) to reflect how important a word is to a document within a collection of documents (corpus).**bold text**"
      ],
      "metadata": {
        "id": "rJmcths-RkcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how it works with an example:\n",
        "\n",
        "Imagine you have a corpus of 3 documents:\n",
        "\n",
        "Document 1: \"The cat sat on the mat.\"\n",
        "Document 2: \"The dog chased the cat.\"\n",
        "Document 3: \"The dog barked loudly.\"\n",
        "\n",
        "Let's calculate the TF-IDF for the word \"cat\":\n",
        "\n",
        "1. Term Frequency (TF):\n",
        "\n",
        "Count how many times \"cat\" appears in each document:\n",
        "Document 1: 1\n",
        "Document 2: 1\n",
        "Document 3: 0\n",
        "Calculate the term frequency (TF) for each document by dividing the count by the total number of words in that document.\n",
        "\n",
        "2. Inverse Document Frequency (IDF):\n",
        "\n",
        "Count how many documents contain the word \"cat\": 2 (Document 1 and Document 2)\n",
        "Calculate IDF: log(Total number of documents / Number of documents containing the word) = log(3/2)\n",
        "\n",
        "3. TF-IDF:\n",
        "\n",
        "Multiply TF and IDF for each document:\n",
        "Document 1: TF(cat) * IDF(cat)\n",
        "Document 2: TF(cat) * IDF(cat)\n",
        "Document 3: 0 (since \"cat\" doesn't appear)\n",
        "Interpretation:\n",
        "\n",
        "A high TF-IDF score means the word is frequent in a specific document but rare across the corpus, indicating its importance for that document.\n",
        "In our example, \"cat\" would have a higher TF-IDF score in Document 1 and 2 compared to other words like \"the\" which appear frequently across all documents.\n",
        "\n",
        "\n",
        "Key Points:\n",
        "\n",
        "TF-IDF helps identify words that are unique and relevant to a specific document within a larger corpus.\n",
        "It's used in various NLP tasks like information retrieval, text classification, and keyword extraction.\n"
      ],
      "metadata": {
        "id": "mQE6b_V5SL_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog chased the cat.\",\n",
        "    \"The dog barked loudly.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print()\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuupTtbDSngh",
        "outputId": "b4d0320b-535f-470e-8a94-e3d946fe701f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['barked' 'cat' 'chased' 'dog' 'loudly' 'mat' 'on' 'sat' 'the']\n",
            "\n",
            "[[0.         0.34101521 0.         0.         0.         0.44839402\n",
            "  0.44839402 0.44839402 0.52965746]\n",
            " [0.         0.40352536 0.53058735 0.40352536 0.         0.\n",
            "  0.         0.         0.62674687]\n",
            " [0.5844829  0.         0.         0.44451431 0.5844829  0.\n",
            "  0.         0.         0.34520502]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Calculation\n",
        "TF (Term Frequency):**\n",
        "\n",
        "The number of times a term appears in a document divided by the total number of terms in that document.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**IDF (Inverse Document Frequency):** Measures how important a term is across the entire corpus. It is calculated as:\n",
        "\n",
        "\n",
        "IDF ( ð‘¡ ) = log â¡ ( ð‘/ DF ( ð‘¡ ) )\n",
        "\n",
        "Where:\n",
        "\n",
        "N is the total number of documents.\n",
        "\n",
        "DF(t) is the number of documents that contain the term\n",
        "t.\n",
        "\n",
        "TF-IDF: The TF-IDF value for a term in a document is calculated as:\n",
        "\n",
        "TF-IDF(t,d)=TF(t,d)Ã—IDF(t)"
      ],
      "metadata": {
        "id": "TxVD0VjmWIxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Limitations of The Bag of Words and TF-IDF**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h1_s43BSaUli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Bag of Words (BoW):\n",
        "\n",
        "**Loss of Word Order and Context:** BoW completely ignores the order of words in a sentence. This means it can't distinguish between \"The cat chased the dog\" and \"The dog chased the cat,\" even though the meaning is completely different.\n",
        "\n",
        "\n",
        "**No Semantic Understanding:** BoW only considers word frequencies and doesn't capture the semantic relationships between words. It treats all words as equally important, regardless of their meaning or context.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "TF-IDF:\n",
        "\n",
        "Limited Contextual Awareness: While TF-IDF considers the rarity of words across documents, it still doesn't fully capture the context in which words are used. For example, it might give high scores to words that are frequent in a specific topic but not necessarily important for understanding the overall meaning of a document.\n",
        "\n",
        "Sparsity: The TF-IDF matrix can become very sparse (mostly filled with zeros) when dealing with large vocabularies, which can pose challenges for some machine learning algorithms.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "General Limitations of Both:\n",
        "\n",
        "Inability to Handle Out-of-Vocabulary Words: Both methods struggle with words that were not present in the training corpus. They can't assign meaningful representations to these new words.\n",
        "\n",
        "Limited Representation of Complex Linguistic Phenomena: BoW and TF-IDF don't capture more complex linguistic features like syntax, negation, or sarcasm, which can be crucial for understanding natural language.\n",
        "\n",
        "To overcome these limitations, more advanced techniques like word embeddings (Word2Vec, GloVe) and deep learning models (RNNs, Transformers) are often used in modern NLP tasks. These methods can capture semantic relationships, context, and even handle out-of-vocabulary words to some extent.\n"
      ],
      "metadata": {
        "id": "tsdPrGhlbLvk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROUmsufradZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word2Vec**\n",
        "\n",
        "Word2Vec is a technique for learning word embeddings, which are dense vector representations of words that capture semantic relationships between them. Unlike BoW and TF-IDF, Word2Vec represents words in a continuous vector space, where words with similar meanings are located closer to each other."
      ],
      "metadata": {
        "id": "1K0aZl3ObPg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "There are two main architectures for Word2Vec:\n",
        "\n",
        "Continuous Bag-of-Words (CBOW): Predicts a target word based on its surrounding context words. It essentially tries to guess a word given its neighbors.\n",
        "\n",
        "Skip-gram: Predicts context words within a certain window given a target word. It tries to guess the neighbors of a given word."
      ],
      "metadata": {
        "id": "0UhDD1OcddKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The below code is not working"
      ],
      "metadata": {
        "id": "zKW1xxjYqfDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KRCeeSsqjHS",
        "outputId": "03f0dd25-daf9-476a-8f5e-96509d1f3697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.2.0 in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.2.0) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "vocab_len = len(wv.key_to_index.keys())\n",
        "print(vocab_len)"
      ],
      "metadata": {
        "id": "qXiXsb40qo-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_rTKjo2qyh3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}