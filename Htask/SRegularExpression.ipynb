{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W42neg36GqMD"
   },
   "source": [
    "#Regular Expressions\n",
    "\n",
    "\n",
    "A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern.\n",
    "\n",
    "RegEx can be used to check if a string contains the specified search pattern.\n",
    "\n",
    "re moducle is used to work with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmhXAJU_HS6X",
    "outputId": "eeacfaac-9bde-4f3b-e6ea-a85e50bd661c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\JODU\n",
      "[nltk_data]     KARTHIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk #Natural language toolkit\n",
    "nltk.download('punkt') #punkt is one of the modules of nltk which focuses on sentence tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-yMElKzIc8c"
   },
   "source": [
    "#Metacharacters\n",
    "\n",
    "A metacharacter is a character that has a sepcial meaning during pattern processing\n",
    "\n",
    "Metacharacters can be used in regular expressions to define the search criteria and any text manipulations.\n",
    "\n",
    "\n",
    "###Some of special Characters\n",
    "\n",
    "<!-- **. (Dot.)** this matches any character except a newline\n",
    "\n",
    "If the DOTALL flag has been specified, this matches any character including a newline. -->\n",
    "\n",
    "**^** --> Beginning of line\n",
    "\n",
    "**$** --> End of line\n",
    "\n",
    "**[abc]** --> Match any character enclosed in the brackets\n",
    "\n",
    "**[^abc]** --> Match any character not enclosed in the brackers\n",
    "\n",
    "**[a-z]** --> Match of range of characters specified by the hypen\n",
    "\n",
    "**.**   --> Match any single character\n",
    "\n",
    "**\\** --> Use the literal meaning of the metacharacter\n",
    "\n",
    "Example : '\\\\.', '\\b'\n",
    "\n",
    "**?** --> Match zero or one of the preceding expression\n",
    "\n",
    "\\* --> match zero,one, or many of the preceding expression\n",
    "\n",
    "\\+ --> Match one or many of the preceding expression\n",
    "\n",
    "\\d --> Match digit (0-9)\n",
    "\n",
    "\\D --> Not a digit (0-9)\n",
    "\n",
    "\\w --> word character\n",
    "\n",
    "\\W --> Not a word character\n",
    "\n",
    "\\s --> whiteSpace(space, tab, newline)\n",
    "\n",
    "\\S  --> Not a whitespace\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WrcKRGqOoZcu"
   },
   "outputs": [],
   "source": [
    "text_to_search = \"\"\"\n",
    "abcdefghijklmnopqrstuvwxyz\n",
    "ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "F\n",
    "123456789\n",
    "\n",
    "Ha HazHa\n",
    "\n",
    "MetaCharacters (need to be excaped)\n",
    ". & $ ^ * + ( ) [] \\ | {}\n",
    "\n",
    "coreyms.com\n",
    "\n",
    "321-555-4321\n",
    "321+555+4321\n",
    "321-555+4321\n",
    "321^555*4321\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuLVJjs2pXJ-"
   },
   "source": [
    "#re.compile()\n",
    "\n",
    "we can combine a regular expression pattern into pattern pattern objects, which can be used for pattern matching. It also helps to search a pattern again without rewriting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tyYXW21pqheG"
   },
   "outputs": [],
   "source": [
    "#defining a pattern\n",
    "\n",
    "text = 'abc is the text here'\n",
    "\n",
    "pattern = re.compile(r'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vki0Uivfqv4B",
    "outputId": "dca6eca7-8186-4c93-be1e-af986dbcd7e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#match\n",
    "\n",
    "match = pattern.match(text)\n",
    "\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSKwHkL6rGHl",
    "outputId": "8a306980-38b7-4f3d-fa5c-de4b194195dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc is the text here\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pattern_m = re.compile(r'here')\n",
    "\n",
    "print(text)\n",
    "\n",
    "match = pattern_m.match(text)\n",
    "\n",
    "print(match) # Because Match() is used to match a pattern at the start of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOul11yBrhZo",
    "outputId": "429b9343-c066-48ba-a1e1-4e04768767fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['abc', 'is', 'the', 'text', 'here']\n"
     ]
    }
   ],
   "source": [
    "#find the matches of pattern\n",
    "#findall method\n",
    "\n",
    "pattern = re.compile(r'\\w+')\n",
    "\n",
    "match = pattern.findall(text)\n",
    "\n",
    "print(type(match))\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2xVI3zvsPcp",
    "outputId": "10d33711-c631-4e28-abe4-722c71503cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='is'>\n"
     ]
    }
   ],
   "source": [
    "#search method\n",
    "#looks for the first location where the pattern matches\n",
    "#If a match is found, the re.search() return a match object. Otherwise, it returns None\n",
    "\n",
    "match = pattern.search(r'is')\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnpUsNQftJHk",
    "outputId": "ee287af0-3f0f-498b-b755-995e2da9140c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "#match period characteer\n",
    "\n",
    "pattern = re.compile(r'\\.')\n",
    "text1 = \"something is useful\"\n",
    "match = pattern.findall(text1)\n",
    "print(match)\n",
    "\n",
    "text1 = \"something is useful.\"\n",
    "match = pattern.findall(text1)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6dm1ndDuH2c",
    "outputId": "f01ddc39-23f8-406e-f988-2507b23328bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123456789',\n",
       " '321',\n",
       " '555',\n",
       " '4321',\n",
       " '321',\n",
       " '555',\n",
       " '4321',\n",
       " '321',\n",
       " '555',\n",
       " '4321',\n",
       " '321',\n",
       " '555',\n",
       " '4321']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#match digits\n",
    "\n",
    "pattern = re.compile(r'\\d+')\n",
    "\n",
    "match = pattern.findall(text_to_search)\n",
    "\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFcrkOsfufzg",
    "outputId": "e34657f4-1c33-4cf1-a6ec-8a69fe4fdab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nabcdefghijklmnopqrstuvwxyz\\nABCDEFGHIJKLMNOPQRSTUVWXYZ\\nF\\n', '\\n\\nHa HazHa\\n\\nMetaCharacters (need to be excaped)\\n. & $ ^ * + ( ) [] \\\\ | {}\\n\\ncoreyms.com\\n\\n', '-', '-', '\\n', '+', '+', '\\n', '-', '+', '\\n', '^', '*', '\\n']\n"
     ]
    }
   ],
   "source": [
    "#match non digits\n",
    "\n",
    "pattern = re.compile(r'\\D+')\n",
    "\n",
    "match = pattern.findall(text_to_search)\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fI67kgMGu2PG",
    "outputId": "248b3d2c-2947-431e-92f9-a91ed8f21bdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz',\n",
       " 'ABCDEFGHIJKLMNOPQRSTUVWXYZ',\n",
       " 'F',\n",
       " '123456789',\n",
       " 'Ha',\n",
       " 'HazHa',\n",
       " 'MetaCharacters',\n",
       " 'need',\n",
       " 'to',\n",
       " 'be',\n",
       " 'excaped',\n",
       " 'coreyms',\n",
       " 'com',\n",
       " '321',\n",
       " '555',\n",
       " '4321',\n",
       " '321',\n",
       " '555',\n",
       " '4321',\n",
       " '321',\n",
       " '555',\n",
       " '4321',\n",
       " '321',\n",
       " '555',\n",
       " '4321']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#match word character\n",
    "\n",
    "pattern = re.compile(r'\\w+')\n",
    "\n",
    "match = pattern.findall(text_to_search)\n",
    "\n",
    "match\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EsN6yNtvn2X",
    "outputId": "b2db2702-8dca-4d1f-dcfe-418355eb8178"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n\\n',\n",
       " ' ',\n",
       " '\\n\\n',\n",
       " ' (',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ')\\n. & $ ^ * + ( ) [] \\\\ | {}\\n\\n',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " '-',\n",
       " '-',\n",
       " '\\n',\n",
       " '+',\n",
       " '+',\n",
       " '\\n',\n",
       " '-',\n",
       " '+',\n",
       " '\\n',\n",
       " '^',\n",
       " '*',\n",
       " '\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#match non word character\n",
    "\n",
    "pattern = re.compile(r'\\W+')\n",
    "\n",
    "match = pattern.findall(text_to_search)\n",
    "\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfqjkmITvviE",
    "outputId": "64f9a1fa-b424-48e5-9ebb-ee4e1a41ad0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Start']\n"
     ]
    }
   ],
   "source": [
    "#beginning of line\n",
    "\n",
    "pattern = re.compile(r'^Start')\n",
    "\n",
    "sentence = \"Start with new end\"\n",
    "\n",
    "match = pattern.findall(sentence)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnbU-95Uv_20",
    "outputId": "9d1d505a-0a90-4379-bf5f-649a4e3e5a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'^start')\n",
    "\n",
    "sentence = \"Start with new end\"\n",
    "\n",
    "match = pattern.findall(sentence)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_E5fHLX0wEsL",
    "outputId": "5c045de3-1654-4073-9a80-a2fe8d63e9ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'^[Ss]tart')\n",
    "\n",
    "sentence = \"start with new end\"\n",
    "\n",
    "match = pattern.findall(sentence)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3haG_U2wYpu",
    "outputId": "02cf07ac-48a0-4b35-9f3b-247736d4119e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['end']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'end$')\n",
    "match = pattern.findall(sentence)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-bGM_eow2d5",
    "outputId": "e59f3e26-3d1f-4823-d03e-9e54c007924c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['321-555-4321', '321+555+4321', '321-555+4321', '321^555*4321']\n"
     ]
    }
   ],
   "source": [
    "#check phone number pattern\n",
    "\n",
    "pattern = re.compile(r'\\d{3}.\\d\\d\\d.\\d{4}') # we can also give like {,3} it is maximum of three elements\n",
    "\n",
    "# {3, } it is minimum of three elements\n",
    "\n",
    "match = pattern.findall(text_to_search)\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S412bcYQxv6Q",
    "outputId": "d2f1ec32-19bf-487f-ca54-588159f8e59e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check phone number with 800 or 900\n",
    "\n",
    "pattern = re.compile(r'[89]00.\\d\\d\\d.\\d\\d\\d\\d')\n",
    "\n",
    "match = pattern.findall(text_to_search)\n",
    "\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cy3v_uKIyBDt",
    "outputId": "4a805777-7ee6-4a6c-d393-f0ebad4a44e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Akhil', 'Mr Anil', 'Ms Lalitha']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check name pattern only starts with Mr\n",
    "\n",
    "pattern = re.compile(r'M[rs]\\.?\\s[A-Z]\\w+')\n",
    "text2 = \"Mr. Akhil Mr Anil Ms Lalitha Mrs Roopa\"\n",
    "match = pattern.findall(text2)\n",
    "match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rBNFpLZzFEp",
    "outputId": "3c35e5a1-d8a3-4054-8758-c09352b3a757"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Akhil', 'Mr Anil', 'Ms Lalitha', 'Mrs Roopa']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check name pattern\n",
    "\n",
    "pattern = re.compile(r'M[rs][s]?\\.?\\s[A-Z]\\w*')\n",
    "\n",
    "match = pattern.findall(text2)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIITuV8pz6Lf",
    "outputId": "78227e5e-cd47-4b7d-b5d3-c9a9460d75b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['srinudarpally@gmail.com', 'jodukarthik@gmail.com', 'shashi@gmail.com']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chack email pattern\n",
    "\n",
    "pattern = re.compile(r'[a-zA-Z]+@[a-zA-Z]+\\.com')\n",
    "emails  = \"\"\"\n",
    "\n",
    "srinudarpally@gmail.com\n",
    "jodukarthik@gmail.com\n",
    "shashi@gmail.com\n",
    "vinay.com\n",
    "\n",
    "\"\"\"\n",
    "match = pattern.findall(emails)\n",
    "\n",
    "match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nASfiAG31l_8"
   },
   "source": [
    "#finditer()\n",
    "\n",
    "Get advantage of *grouping*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7G5POOaD1qHO",
    "outputId": "452ad4cb-3902-4d03-bd14-51bc9c806a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 9), match='Mr. Akhil'>\n",
      "<re.Match object; span=(10, 17), match='Mr Anil'>\n",
      "<re.Match object; span=(18, 28), match='Ms Lalitha'>\n",
      "<re.Match object; span=(29, 38), match='Mrs Roopa'>\n"
     ]
    }
   ],
   "source": [
    "#check name pattern\n",
    "\n",
    "pattern = re.compile(r'(Ms|Mr|Mrs)\\.?\\s[A-Z]\\w*')\n",
    "\n",
    "matches = pattern.finditer(text2)\n",
    "\n",
    "for match in matches:\n",
    "  print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUKFZoEg2iSZ",
    "outputId": "a09eccbb-29ca-46c0-c5f7-e3218befe22f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 25), match='srinudarpally@gmail.com'>\n",
      "<re.Match object; span=(26, 47), match='jodukarthik@gmail.com'>\n",
      "<re.Match object; span=(48, 64), match='shashi@gmail.com'>\n"
     ]
    }
   ],
   "source": [
    "#check email pattern\n",
    "\n",
    "pattern = re.compile(r'[a-zA-Z0-9.-]+@[a-zA-Z-]+\\.(com|net|edu)')\n",
    "\n",
    "matches = pattern.finditer(emails)\n",
    "for match in matches:\n",
    "  print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I_f6rf23U38"
   },
   "source": [
    "#Tokenization\n",
    "\n",
    "The process of segmenting running text into linguistic units such as words, punctuation, numbers, alpha-numberics, etc. This process is called tokenization.\n",
    "\n",
    "###Sentence Tokenization\n",
    "(spliting string into sentences)\n",
    "###Word tokenization\n",
    "(splitting string into meaningul words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Q2Gf7keM4EA9"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"In python, we use nltk ( natural language toolkit) for its implementation. punkt is one of the modules in nltk.\n",
    " Punkt is made to learn parameters from a corpus in an unsupervised way that is related to the target domain, such as a list of abbreviations, acronyms, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_KfOOjH42MN",
    "outputId": "8250ca77-cdc7-45b7-adb9-19504d30772e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In python, we use nltk ( natural language toolkit) for its implementation',\n",
       " ' punkt is one of the modules in nltk',\n",
       " '\\n Punkt is made to learn parameters from a corpus in an unsupervised way that is related to the target domain, such as a list of abbreviations, acronyms, etc',\n",
       " '']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence Tokenization\n",
    "\n",
    "sentences = text.split('.')\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkPhO-7Z5Ven",
    "outputId": "bc9946ee-89f8-41e1-f1d9-33312df90455"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'python,',\n",
       " 'we',\n",
       " 'use',\n",
       " 'nltk',\n",
       " '(',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'toolkit)',\n",
       " 'for',\n",
       " 'its',\n",
       " 'implementation.',\n",
       " 'punkt',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'modules',\n",
       " 'in',\n",
       " 'nltk.',\n",
       " 'Punkt',\n",
       " 'is',\n",
       " 'made',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'parameters',\n",
       " 'from',\n",
       " 'a',\n",
       " 'corpus',\n",
       " 'in',\n",
       " 'an',\n",
       " 'unsupervised',\n",
       " 'way',\n",
       " 'that',\n",
       " 'is',\n",
       " 'related',\n",
       " 'to',\n",
       " 'the',\n",
       " 'target',\n",
       " 'domain,',\n",
       " 'such',\n",
       " 'as',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'abbreviations,',\n",
       " 'acronyms,',\n",
       " 'etc.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word Tokenization\n",
    "\n",
    "words = text.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ismfo1hs6XYa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbArSCr85ovy"
   },
   "source": [
    "#Stop Words\n",
    "\n",
    "In computing, stop words are words that are filtered out before or after the natural language data(text) are processed. While \"stop words\" typically refers to them ost common words in a language, all-natural laguage processing tools don't use a single universal list of stop words.\n",
    "\n",
    "which are not contributing the actual meaning to the sentence.\n",
    "\n",
    "ex : ['the', 'is', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3Cx3fZm6TY_",
    "outputId": "b5e3a45f-a662-43ca-b984-95af057e78f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh,', 'man,', 'this', 'is', 'pretty', 'cool,', 'we', 'will', 'do', 'more', 'such', 'things.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['oh,', 'man,', 'pretty', 'cool,', 'do', 'more', 'things.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define you own stop words\n",
    "\n",
    "stopwords = ['a', 'the', 'we','such', 'this', 'an', 'will', 'is']\n",
    "\n",
    "text = \"oh, man, this is pretty cool, we will do more such things.\"\n",
    "\n",
    "words = text.lower().split()\n",
    "\n",
    "print(words)\n",
    "\n",
    "words_filtered = [w for w in words if not w in stopwords]\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rju8pKNa7n4s"
   },
   "source": [
    "#Lemmatization\n",
    "\n",
    "Getting root word from the actual words\n",
    "\n",
    "\"walked\", \"waking\", \"walk\"\n",
    "\n",
    "root word : walk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeF5gP5V8H4x",
    "outputId": "53d26b50-8256-4717-a1c6-5af52df92979"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\JODU\n",
      "[nltk_data]     KARTHIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import required moduel for lemmatiztion\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpA_UhRp8yUb",
    "outputId": "ce38e35d-114e-4774-b338-4958e1cf9752"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\JODU KARTHIK/nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\JODU KARTHIK/nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11876\\3531775190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cats :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cats\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"feet : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"feet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"walks : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"walks\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1174\u001b[0m             )\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprovenances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0momw_prov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# A cache to store the wordnet data of multiple languages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36momw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[0mprovdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0mprovdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eng\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_omw_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[0mprov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\JODU KARTHIK/nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\anaconda3\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\JODU KARTHIK\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"cats :\", lemmatizer.lemmatize(\"cats\"))\n",
    "print(\"feet : \", lemmatizer.lemmatize(\"feet\"))\n",
    "print(\"walks : \", lemmatizer.lemmatize(\"walks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u0gfGwQ9aro"
   },
   "source": [
    "#Stemming\n",
    "\n",
    "It is to remove suffix or prefixes.\n",
    "\n",
    "ex:  eating -> eat\n",
    "     eaten -> eat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30ilVtK99qwl",
    "outputId": "eb0b7b7e-2dd2-4af0-a4a8-db7f3ade96f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feet :  feet\n",
      "likely :  like\n",
      "walking :  walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print('feet : ', stemmer.stem('feet'))\n",
    "print('likely : ', stemmer.stem('likely'))\n",
    "print('walking : ', stemmer.stem('walking'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGT1Q2Yg-xKm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
